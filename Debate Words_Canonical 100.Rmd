---
title: "Exploring GloVe 100d Word Vectors in R"
author: "WW44SS"
date: "March 18, 2016"
output: 
    html_document:
        theme: united
---

####SUMMARY
 
Word vector rely on deep-learning based word-correlation analysis of texts with ~ billions of words to provide interesting linear structures in the word-vector space which convey relationships between words. This analysis explores some of the "canonical" examples of word vector relations - mostly as an exercise to see if I can reproduce known results before embarking on my own anlaysis.  

An interesting finding is that while the scalar product of two vectors reveals the $\cos$ difference between vectors, there appears to be more information in looking at the similarity of a spectrum of _like_ words. 

####DATA SOURCES AND METHODS
 
The deep learning for word-vector analysis relies on the "pre-trained" __GloVe__ vectors from [Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014](http://nlp.stanford.edu/pubs/glove.pdf). 

```{r, echo=FALSE}
## directory

directory <- "/Users/winstonsaunders/Documents/Presidential_Debates_2015/"

## libraries

library(ggplot2)
library(plyr)
library(reshape2)
```



<style>
  .col2 {
    columns: 2 300px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 300px; /* chrome, safari */
    -moz-columns: 2 300px;    /* firefox */
  }
  .col3 {
    columns: 3 200px;
    -webkit-columns: 3 200px;
    -moz-columns: 3 200px;
  }
</style>



<style>
tr:hover {background-color: #BBFFFF}
table { 
    width: 80%;
    display: table;
    border-collapse: collapse;
    border-spacing: 18px;
    border-color: #AAAAFF;
    background-color: #AFEEEE;
    padding: 2px;
    font: 12px arial, sans-serif;
}
th, td{
    text-align: center;
}
</style>




###WORD VECTOR ANALYSIS

The first step is to load the GloVe word vectors.  

```{r echo=9:10}

## GET WORD VECTORS AND WORD LIST
##  creates a vector of words and a vector table
## INPUTS
##      read the lines of the glove vectors
## OUTPUTS
##      word.list - a vector of words

number.of.words<-9900
word.vector.file<-"glove.6B.100d.txt"

word.vectors <- readLines(paste0(directory, word.vector.file), n=number.of.words)
## this is what the data look like a l
## [1] "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095..."
##

## splits the data into individual elements
word.vector.list<-matrix(strsplit(word.vectors, " "), nrow=length(word.vectors), byrow = TRUE)
## now the data look like
## > word.vector.list[1]
## [[1]]
##[1] "the"         "0.418"       "0.24968"     "-0.41242"    "0.1217"      "0.34527"     "-0.044457"   "-0.49688"   
##[9] "-0.17862"    "-0.00066023" "-0.6566"     "0.27843"     "-0.14767"    "-0.55677"    "0.14658" ...    "-0.0095095" 

#length(word.vector.list)

## put the vector into a data frame
tstart <- Sys.time()
word.vector.df<-data.frame(matrix(unlist(word.vector.list), nrow=length(word.vector.list), byrow=T))
tdataframe <- Sys.time()-tstart
##
## The data frame now looks like 
##
#    X1       X2        X3       X4       X5      X6        X7       X8       X9         X10       X11      X12
# 1 the    0.418   0.24968 -0.41242   0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023   -0.6566  0.27843
# 2   , 0.013441   0.23682 -0.16899  0.40951 0.63812   0.47709 -0.42852 -0.55641      -0.364  -0.23938  0.13001
# 3   .  0.15164   0.30177 -0.16763  0.17684 0.31719   0.33973 -0.43478 -0.31086    -0.44999  -0.29486  0.16608
# 4  of  0.70853   0.57088  -0.4716  0.18048 0.54449   0.72603  0.18157 -0.52393     0.10381  -0.17566 0.078852
# 5  to  0.68047 -0.039263  0.30186 -0.17792 0.42962  0.032246 -0.41376  0.13228    -0.29847 -0.085253  0.17118
# 6 and  0.26818   0.14346 -0.27877 0.016257 0.11384   0.69923 -0.51332 -0.47368    -0.33075  -0.13834   0.2702


## format the df
    word.vector.df[,1]<-as.character(word.vector.df[,1])
    for(i in 2:ncol(word.vector.df))     word.vector.df[,i]<-as.numeric(as.character(word.vector.df[,i]))
    ##
    ## here is what the df looks like
    #    X1       X2        X3       X4        X5      X6        X7       X8       X9         X10       X11      X12       X13      X14
    # 1 the 0.418000  0.249680 -0.41242  0.121700 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.656600 0.278430 -0.147670 -0.55677
    # 2   , 0.013441  0.236820 -0.16899  0.409510 0.63812  0.477090 -0.42852 -0.55641 -0.36400000 -0.239380 0.130010 -0.063734 -0.39575
    # 3   . 0.151640  0.301770 -0.16763  0.176840 0.31719  0.339730 -0.43478 -0.31086 -0.44999000 -0.294860 0.166080  0.119630 -0.41328
    # 4  of 0.708530  0.570880 -0.47160  0.180480 0.54449  0.726030  0.18157 -0.52393  0.10381000 -0.175660 0.078852 -0.362160 -0.11829
    # 5  to 0.680470 -0.039263  0.30186 -0.177920 0.42962  0.032246 -0.41376  0.13228 -0.29847000 -0.085253 0.171180  0.224190 -0.10046
    # 6 and 0.268180  0.143460 -0.27877  0.016257 0.11384  0.699230 -0.51332 -0.47368 -0.33075000 -0.138340 0.270200  0.309380 -0.45012

## assign separate list of words
    word.list<-word.vector.df[,1]

    ##[1] "the" ","   "."   "of"  "to"  "and" "in"  "a"   "\""  "'s"

```


The analysis uses the first `r number.of.words` of the GloVe word vectors `r word.vector.file`. It took `r signif(tdataframe, 2)` seconds to process the data into a data frame.

####WORD VECTOR SIMILARITY

Before delving into the full analysis, it's helpful to look at vector representations of familiar word-pairs to help build an intuitive sense of how vector representations of words facilitate their coparison and analysis.  

Vectors of individual words are easily extracted from the word vector data frame using simple _regular expressions_ syntax.

```{r, echo=FALSE}
    
    ## Some sentiment vectors
    ##
    
    vector.normalize <- function(x){
        ##
        ## a function that produces a normal vector parallel to vector x
        ## INPUTS:
        ##    x: a vector
        ## OUTPUTS:
        ##    normal.x: a normal vector parallel to x
        
        if (sum(abs(x)) > 0 ) normal.x <- x / sqrt(sum(x*x)) 
            else normal.x<-0
        return(normal.x)
        
        
    }
```

```{r, eval=FALSE}
man.vec <- vector.normalize( colSums( word.vector.df[ grep( "^man$", word.list), -1] ) )
```

The vector $\textbf{man.vec}$ is normalized to unit length representing the word _`r word.list[grep("^man$", word.list)]`_.

There is an embedded colSums command, irrelevant in this specific case, to ensure all selected vectors are added in case of a more general regular expression search. This generaization isn't useful right now, but may be useful later. For example, selecting strings starting with _"govern"_ 

```{r, eval=FALSE}
govern.vec <- vector.normalize( colSums( word.vector.df[ grep( "^govern", word.list), -1] ) )
```


```{r, echo=FALSE}
govern.words <- word.list[grep("^govern", word.list)]

```

creates a unit vector representing the following `r length(govern.words)` words: _`r paste(govern.words[1:(length(govern.words)-1)], sep="", collapse=", ")`_*,* and _`r govern.words[length(govern.words)]`_. 

```{r, echo=FALSE}



    ## Compute some individual word vectors
    ## Assumes the word.list and word.vector.df data frames already exist

    ##FEAR
        fear.words <- word.list[grep("^fear$", word.list)]
        fear.vec <- vector.normalize(colSums(word.vector.df[grep("^fear$", word.list),-1]))
       
    ## COURAGE    
        courage.words<-word.list[grep("^courag", word.list)]
        courage.vec<-colSums(word.vector.df[grep("^courag", word.list),-1])
        courage.vec <- vector.normalize (courage.vec)
    ## BRAVE    
        brave.words<-word.list[grep("brave", word.list)]
        brave.vec<-colSums(word.vector.df[grep("brave", word.list),-1])
        brave.vec <- vector.normalize (brave.vec)
    ## ANGER    
        anger.words<-c(word.list[grep("^anger", word.list)], word.list[grep("angry", word.list)] )
        anger.vec<-colSums(word.vector.df[grep("^anger", word.list),-1])
        anger.vec<-vector.normalize(anger.vec)
    ## COMPASSION    
        compassion.words<-word.list[grep("^sympath", word.list)]
        compassion.vec<-colSums(word.vector.df[grep("^sympath", word.list),-1])
        compassion.vec<-vector.normalize(compassion.vec)
    ## PRIDE
        pride.words<-word.list[grep("^proud", word.list)]
        pride.vec<-colSums(rbind
                           (word.vector.df[grep("^pride", word.list),-1],word.vector.df[grep("^proud", word.list),-1]))
        pride.vec<-vector.normalize(pride.vec)
    ## MAD
        mad.words<-word.list[grep("^mad", word.list)]
        mad.vec<-colSums(rbind
                           (word.vector.df[grep("^mad", word.list),-1],word.vector.df[grep("^mad", word.list),-1]))
        mad.vec<-vector.normalize(mad.vec)   
    ## SCARED
        scare.words<-word.list[grep("^scare", word.list)]
        scare.vec<-colSums(rbind
                           (word.vector.df[grep("^scare", word.list),-1],word.vector.df[grep("^scare", word.list),-1]))
        scare.vec<-vector.normalize(scare.vec) 
    ## GOVERN
        govern.words<-word.list[grep("^govern", word.list)]
        govern.vec<-vector.normalize(colSums(rbind
                           (word.vector.df[grep("^govern", word.list),-1],word.vector.df[grep("^govern", word.list),-1])))
        president.vec<-vector.normalize(colSums(rbind
                           (word.vector.df[grep("^president$", word.list),-1],word.vector.df[grep("^president$", word.list),-1])))
        government.vec<-vector.normalize(colSums(rbind
                           (word.vector.df[grep("^government", word.list),-1],word.vector.df[grep("^government", word.list),-1])))
    ## TAX
        tax.words<-word.list[grep("^tax", word.list)]
        tax.vec<-vector.normalize(colSums(word.vector.df[grep("^tax", word.list),-1]))
    ## PEOPLE
        people.words<-word.list[grep("^people", word.list)]
        people.vec<-vector.normalize(colSums(rbind
                           (word.vector.df[grep("^people", word.list),-1],word.vector.df[grep("^people", word.list),-1])))
    
    ## AMERICA
        america.words<-word.list[grep("^america", word.list)]
        america.vec<-vector.normalize(colSums(word.vector.df[grep("^america", word.list),-1]))
        
        
        country.vec<-vector.normalize(colSums(word.vector.df[grep("^country$", word.list),-1]))
        nation.vec<-vector.normalize(colSums(word.vector.df[grep("^nation$", word.list),-1]))
        
    ## RELATED WORDS
        man.vec<-vector.normalize(colSums(word.vector.df[grep("^man$", word.list),-1]))
        man.words<-word.list[grep("^man$", word.list)]
        
        male.vec<-vector.normalize(colSums(word.vector.df[grep("^male$", word.list),-1]))
        #word.list[grep("^male$", word.list)]
        
        female.vec<-vector.normalize(colSums(word.vector.df[grep("^female$", word.list),-1]))
        #word.list[grep("^female$", word.list)]
        
        woman.vec<-vector.normalize(colSums(word.vector.df[grep("^woman$", word.list),-1]))
        #word.list[grep("^woman$", word.list)]
         
        aunt.vec<-vector.normalize(colSums(word.vector.df[grep("^aunt$", word.list),-1]))
        #word.list[grep("^aunt$", word.list)]
         
        uncle.vec<-vector.normalize(colSums(word.vector.df[grep("^uncle$", word.list),-1]))
        #word.list[grep("^uncle$", word.list)]
         
        king.vec<-vector.normalize(colSums(word.vector.df[grep("^king$", word.list),-1]))
        #word.list[grep("^king$", word.list)]
         
        queen.vec<-vector.normalize(colSums(word.vector.df[grep("^queen$", word.list),-1]))
        #word.list[grep("^queen$", word.list)]
        
        mother.vec<-vector.normalize(colSums(word.vector.df[grep("^mother$", word.list),-1]))
        #word.list[grep("^queen$", word.list)]
        
        father.vec<-vector.normalize(colSums(word.vector.df[grep("father$", word.list),-1]))
        #word.list[grep("^queen$", word.list)]
        
        girl.vec<-vector.normalize(colSums(word.vector.df[grep("girl$", word.list),-1]))
        #word.list[grep("^queen$", word.list)]
        
        boy.vec<-vector.normalize(colSums(word.vector.df[grep("boy$", word.list),-1]))
        #word.list[grep("^queen$", word.list)]
        
        his.vec<-vector.normalize(colSums(word.vector.df[grep("his$", word.list),-1]))
        #word.list[grep("^queen$", word.list)]
        
        hers.vec <- vector.normalize(colSums(word.vector.df[grep("hers$", word.list), -1] ))
        #word.list[grep("^queen$", word.list)]
        
        oregon.vec <- vector.normalize(colSums(word.vector.df[grep("^oregon$", word.list), -1] ))
        salem.vec <- vector.normalize(colSums(word.vector.df[grep("^salem$", word.list), -1] ))
        nevada.vec <- vector.normalize(colSums(word.vector.df[grep("^nevada$", word.list), -1] ))
        reno.vec <- vector.normalize(colSums(word.vector.df[grep("^reno$", word.list), -1] ))
        colorado.vec <- vector.normalize(colSums(word.vector.df[grep("^colorado$", word.list), -1] ))
        denver.vec <- vector.normalize(colSums(word.vector.df[grep("^denver$", word.list), -1] ))
        texas.vec <- vector.normalize(colSums(word.vector.df[grep("^texas$", word.list), -1] ))
        austin.vec <- vector.normalize(colSums(word.vector.df[grep("^austin$", word.list), -1] ))
        seattle.vec <- vector.normalize(colSums(word.vector.df[grep("^seattle$", word.list), -1] ))
        portland.vec <- vector.normalize(colSums(word.vector.df[grep("^portland$", word.list), -1] ))
        california.vec <- vector.normalize(colSums(word.vector.df[grep("^california$", word.list), -1] ))
        sacramento.vec <- vector.normalize(colSums(word.vector.df[grep("^sacramento$", word.list), -1] ))
        
``` 

####UNDERSTANDING WORD VECTORS

Before diving in to comparisons, it's helpful to reproduce an well-known and intuitively appealing example of word vector comparisons. Looking at the vectors of multiple words with a male-female relationship, we can see how the relationships develop.  
The example below approximately reproduces results [here](http://nlp.stanford.edu/projects/glove/images/man_woman.jpg) ). The postion of each word represents the "vector" of that word (noting that this is a two dimenstional representation of a `r length(man.vec)` element vector)

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=4}

## 
## THIS CODE CHUNK
## computes the principal components of two word vectors and then plots word vectors in relation to these
## it assumes word vectors are already calculated
    
    ## compose word vector df
    wordvec.df<-as.data.frame(rbind(man.vec, woman.vec))
    row.names(wordvec.df)<-c("man", "woman")

#wordvec.df<-as.data.frame(rbind(aunt.vec, uncle.vec))
#row.names(wordvec.df)<-c("aunt", "uncle")

#wordvec.df
 
a<-prcomp(wordvec.df, scale.=TRUE, center=TRUE)

pcdf1<- as.data.frame (rbind (
        predict(a, rbind(NULL,man.vec)),
        predict(a, rbind(NULL,woman.vec)),
        predict(a, rbind(NULL,aunt.vec)),
        predict(a, rbind(NULL,uncle.vec)),
        predict(a, rbind(NULL,king.vec)),
        predict(a, rbind(NULL,queen.vec)),
        predict(a, rbind(NULL,male.vec)),
        predict(a, rbind(NULL,female.vec))
       
       ))

pcdf1$word<- gsub(".vec", "", row.names(pcdf1))

ggplot(pcdf1, aes(x=PC2, y=PC1, color=word, label=word)) + 
    scale_x_continuous(expand=c(.2,0)) +
    scale_y_continuous(expand=c(.2,0)) + 
    theme(legend.position="none") + labs(x="", y="") +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="man"], y = pcdf1$PC1[pcdf1$word=="man"], xend = pcdf1$PC2[pcdf1$word=="woman"], yend = pcdf1$PC1[pcdf1$word=="woman"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1) +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="aunt"], y = pcdf1$PC1[pcdf1$word=="aunt"], xend = pcdf1$PC2[pcdf1$word=="uncle"], yend = pcdf1$PC1[pcdf1$word=="uncle"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1)  +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="male"], y = pcdf1$PC1[pcdf1$word=="male"], xend = pcdf1$PC2[pcdf1$word=="female"], yend = pcdf1$PC1[pcdf1$word=="female"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1) +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="king"], y = pcdf1$PC1[pcdf1$word=="king"], xend = pcdf1$PC2[pcdf1$word=="queen"], yend = pcdf1$PC1[pcdf1$word=="queen"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1) +
    geom_text(size=5)


```

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=4}

wordvec.df<-as.data.frame(rbind(male.vec, female.vec))
row.names(wordvec.df)<-c("male", "female")

#wordvec.df<-as.data.frame(rbind(aunt.vec, uncle.vec))
#row.names(wordvec.df)<-c("aunt", "uncle")

#wordvec.df
 
a<-prcomp(wordvec.df, scale.=TRUE, center=TRUE)

pcdf1<- as.data.frame (rbind (
        predict(a, rbind(NULL,man.vec)),
        predict(a, rbind(NULL,woman.vec)),
        predict(a, rbind(NULL,aunt.vec)),
        predict(a, rbind(NULL,uncle.vec)),
        predict(a, rbind(NULL,king.vec)),
        predict(a, rbind(NULL,queen.vec)),
        predict(a, rbind(NULL,male.vec)),
        predict(a, rbind(NULL,female.vec))
       
       ))

pcdf1$word<- gsub(".vec", "", row.names(pcdf1))

ggplot(pcdf1, aes(x=PC2, y=PC1, color=word, label=word)) + 
    scale_x_continuous(expand=c(.2,0)) +
    scale_y_continuous(expand=c(.2,0)) + 
    theme(legend.position="none") + labs(x="", y="") +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="man"], y = pcdf1$PC1[pcdf1$word=="man"], xend = pcdf1$PC2[pcdf1$word=="woman"], yend = pcdf1$PC1[pcdf1$word=="woman"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1) +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="aunt"], y = pcdf1$PC1[pcdf1$word=="aunt"], xend = pcdf1$PC2[pcdf1$word=="uncle"], yend = pcdf1$PC1[pcdf1$word=="uncle"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1)  +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="male"], y = pcdf1$PC1[pcdf1$word=="male"], xend = pcdf1$PC2[pcdf1$word=="female"], yend = pcdf1$PC1[pcdf1$word=="female"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1) +
    geom_segment(aes(x = pcdf1$PC2[pcdf1$word=="king"], y = pcdf1$PC1[pcdf1$word=="king"], xend = pcdf1$PC2[pcdf1$word=="queen"], yend = pcdf1$PC1[pcdf1$word=="queen"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1) +
    geom_text(size=5)


```




Recalling basic properties of [vector addition](http://mathworld.wolfram.com/VectorAddition.html), we can see from the above that the approximate expressions seem to hold:

$$
\begin{aligned}
\textbf{male} + \textbf{woman} - \textbf{female} \approx \textbf{man}
\end{aligned}
$$ 

$$
\begin{aligned}
\textbf{man} + \textbf{queen} - \textbf{woman} \approx \textbf{king}
\end{aligned}
$$

While these identities are not fully satisfied in the two dimensional representation of the `r length(man.vec)` element word vectors, we can quantify differences more precisely by taking the quantity 

$$
\begin{aligned} 
\cos{\theta_{d}} =  \| \textbf{pseudo.d} \cdot \textbf{d} \| / ( \| \textbf{d} \| \| \textbf{pseudo.d} \| ))
\end{aligned}
$$

where

$$
\begin{aligned}
\textbf{pseudo.d} \equiv \textbf{a} + \textbf{b} - \textbf{c} \approx \textbf{d}
\end{aligned}
$$



```{r, echo=FALSE}

pseudo.king.vec<-man.vec + queen.vec - woman.vec

pseudo.king.error <- abs(sum(pseudo.king.vec * king.vec))/(sqrt(sum(king.vec * king.vec))* sqrt(sum(pseudo.king.vec * pseudo.king.vec)))



pseudo.man.vec<-male.vec + woman.vec - female.vec

pseudo.man.error <- abs(sum(pseudo.man.vec * man.vec))/(sqrt(sum(man.vec * man.vec))*sqrt(sum(pseudo.man.vec * pseudo.man.vec)))

```

In this case $\cos{\theta_{king}}$ = `r round(pseudo.king.error, 3)` and the $\cos{\theta_{man}}$ = `r round(pseudo.man.error, 3)`. The question is, is this good or not good?

####VECTORS OF COMPOUNDED WORDS

To get an idea of the significance of compounding wrods, let's compare the similarity of the pseudo.vector to the word itself.   

As a first stab we can look at the vectors themselves as shown below. The plot simply encodes the value of each vector component. While the vector for $\textbf{king}$ and $\textbf{pseudo.king}$ are similar, it's hard, at least for for me, to distinguish that the two vectors have very much in common. For instance, $\textbf{king}$ any closer to $\textbf{woman}$ than to $\textbf{man}$? In retrospect this is not surprising. The vectors are highly abstracted representations that make sense only in the context of other words. 

```{r, echo=FALSE, fig.align="center", fig.width=8.5, fig.height=1.5}

    display.vec<-rbind(pseudo.king.vec, king.vec, man.vec, queen.vec, woman.vec)

    #display.vec$names<-c("cruz", "rubio", "clinton", "sanders", "trump")
    
    m1<-melt(display.vec)
    
    p <- ggplot(m1, aes(y=Var1, x=Var2))
    p<- p + geom_tile(aes(fill=value), color="white")
    p<- p + scale_fill_gradient2(low = "darkred", mid = "paleturquoise", high = "darkgreen", midpoint=0, na.value="grey90")
    p <- p + theme(axis.text.x = element_blank(),axis.ticks = element_blank(), legend.position="none")
    p <- p + xlab("")
    
    p
```

####WORD TO WORD COMPARISON IS MORE INTERESTING

It turns out it's much more interesting  to look at the vectors not in the abstract, but in the context of actual words. The plots below I've taken the scalar product of the vectors $\textbf{king}$ and $\textbf{pseudo.king}$ with $\textbf{king}$ and some neighboring words (for all intents, random).

```{r, echo=FALSE, fig.align="center", fig.width=8.5, fig.height=3.5}
#this code chunk creates a graph of the alignment of vectors near a specific pseudo.vector

    # find position of the chosen word
    pos<-grep("^king$", word.list)

    ## plot only if a finite subset
    if (length(pos) == 1) {
    
    half.width<-44    
    plot.word.df <- word.vector.df[max(1,(pos-half.width)):min(nrow(word.vector.df),(pos+half.width)),-1]
    
    
    start.dup<-max(1,(pos-half.width))
    end.dup <- min(nrow(word.vector.df),(pos+half.width))
    n.dup<-end.dup-start.dup+1
    
    
    ## replicate pseuodvector
    
    pseudo.king.df<-do.call("rbind", replicate(n.dup, pseudo.king.vec/sqrt(sum(pseudo.king.vec*pseudo.king.vec)), simplify = FALSE))
    king.df<-do.call("rbind", replicate(n.dup, king.vec/sqrt(sum(king.vec*king.vec)), simplify = FALSE))
    man.df<-do.call("rbind", replicate(n.dup, man.vec/sqrt(sum(man.vec*man.vec)), simplify = FALSE))
    
    
    
    pseudo.dot.product<-abs(rowSums(plot.word.df*pseudo.king.df))/sqrt(rowSums(plot.word.df*plot.word.df))
    king.dot.product<-abs(rowSums(plot.word.df*king.df))/sqrt(rowSums(plot.word.df*plot.word.df))
    man.dot.product<-abs(rowSums(plot.word.df*man.df))/sqrt(rowSums(plot.word.df*plot.word.df))
    word.list.temp<-word.list[start.dup:end.dup]
    
    ## form plot data frame comparing pseudo vector and vector
    
    plot.temp<-cbind(as.data.frame(word.list.temp), as.data.frame(king.dot.product), as.data.frame(pseudo.dot.product))
    melted.plot.temp <- melt(plot.temp, id="word.list.temp")
    
    library(graphics)
    
    
    p <- ggplot(melted.plot.temp, aes(x=word.list.temp, y = value, fill=variable))
    p <- p + geom_bar(stat="identity", position="dodge")
    p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5), legend.position="top")
    p <- p + xlab("")
    p1 <- p + ylab(expression(paste("cos",theta)))
    
    
    print(p1)
    
    
    ## compare pseudo.vector to root
    
    plot.temp<-cbind(as.data.frame(word.list.temp), as.data.frame(king.dot.product), as.data.frame(man.dot.product))
    melted.plot.temp <- melt(plot.temp, id="word.list.temp")
    
    
    p <- ggplot(melted.plot.temp, aes(x=word.list.temp, y = value, fill=variable))
    p <- p + geom_bar(stat="identity", position="dodge")
    p <- p + theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust=0.5), legend.position="top")
    p <- p + xlab("")
    p2 <- p + ylab(expression(paste("cos",theta)))
    
   
    
    
    }

```

Here we take just a sample of the nearest `r 2*half.width`  neighboring words. Note that both the $\textbf{king}$ and $\textbf{pseudo.king}$ have a high scalar product with the vector representing $\textbf{king}$, whereas the root vector $\textbf{man}$ does not.  
Another point is the relatively higher scalar products of the __pseudo.vector__ with words like _"britain"_ and _"george"_ also stand out. This is important to recognize in relation to "compounded meaning" of text in relation to specific words.  
As a comparison the plot below shows the scalar products for $\textbf{king}$ and $\textbf{man}$ with neighboring words. This reveals the scope of the change in the vector, and some sense the _meaning_ of the word.

```{r, echo=FALSE, fig.align="center", fig.width=8.5, fig.height=3.5}

print(p2)

```

####CITY STATE PAIRS

This was just a trial to test how capitol and state paris lined up. It's interesting that California and Sacramento are _"upside down"_ in the 300d version of the vector, but in the right order here.

```{r, echo=FALSE, fig.align="center", fig.height=4, fig.width=4}

## 
## THIS CODE CHUNK
## computes the principal components of two word vectors and then plots word vectors in relation to these
## it assumes word vectors are already calculated
    
    ## compose word vector df
    wordvec.df<-as.data.frame(rbind(nevada.vec, reno.vec))
    row.names(wordvec.df)<-c("nevada", "reno")



    ## compute principal components
    a<-prcomp(wordvec.df, scale.=TRUE, center=TRUE)

    ## define plot data frame
    pcdf1<- as.data.frame (rbind (
        predict(a, rbind(NULL,nevada.vec)),
        predict(a, rbind(NULL,texas.vec)),
        predict(a, rbind(NULL,reno.vec)),
        predict(a, rbind(NULL,austin.vec)),
        predict(a, rbind(NULL,california.vec)),
        predict(a, rbind(NULL,sacramento.vec))
       
       ))
    
    ## label rows
    pcdf1$word<- gsub(".vec", "", row.names(pcdf1))

    ## plot
    ggplot(pcdf1, aes(x=PC2, y=PC1, color=word, label=word)) + 
        scale_x_continuous(expand=c(.2,0)) +
        scale_y_continuous(expand=c(.2,0)) + 
        theme(legend.position="none") + labs(x="", y="") +
        geom_segment( aes(x = pcdf1$PC2[pcdf1$word=="california"], y = pcdf1$PC1[pcdf1$word=="california"], xend = pcdf1$PC2[pcdf1$word=="sacramento"], yend = pcdf1$PC1[pcdf1$word=="sacramento"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1 ) +
        geom_segment( aes(x = pcdf1$PC2[pcdf1$word=="texas"], y = pcdf1$PC1[pcdf1$word=="texas"], xend = pcdf1$PC2[pcdf1$word=="austin"], yend = pcdf1$PC1[pcdf1$word=="austin"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1 )  +
        geom_segment( aes(x = pcdf1$PC2[pcdf1$word=="nevada"], y = pcdf1$PC1[pcdf1$word=="nevada"], xend = pcdf1$PC2[pcdf1$word=="reno"], yend = pcdf1$PC1[pcdf1$word=="reno"]), color="darkblue", linetype = "dashed", size=0.05, data=pcdf1 ) +
        geom_text( size=5 )


```



####CONCLUSIONS

It's relatively easy to reproduce "canonical" examples of word vector relations. The work as advertised.

The value of the scalar product of two word vectors seems to convey less information than the _context_ of the scalar product, i.e. there appears to be more information in comparing a _spectrum_ of words. This suggests a metric of _goodness_ might be synthesized from something like this nearest neighbor comparison. Something to explore. 




